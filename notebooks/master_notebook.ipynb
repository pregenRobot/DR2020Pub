{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Checklist\n",
    "\n",
    "NOTE: (Tested) means the checkpoint was carried out\n",
    "\n",
    "0. Some data visualizations (Tested) \n",
    "\n",
    "1. Principle Component Analysis\n",
    "\n",
    "2. Locally Linear Embedding\n",
    "\n",
    "3. Multidimensional Scaling\n",
    "\n",
    "4. Isopmap\n",
    "\n",
    "5. t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "6. Linear Discriminant Analysis\n",
    "\n",
    "7. Correlation Metrics (Tested)\n",
    "    \n",
    "    a. Pearson (Tested)\n",
    "    \n",
    "    b. Kendall (Tested)\n",
    "    \n",
    "    c. Spearman (Tested)\n",
    "    \n",
    "    \n",
    "8. Chi Squared Test (Tested)\n",
    "\n",
    "9. Recurisve Feature Elimination\n",
    "\n",
    "10. Lasso (Regression) : SelectFromModel\n",
    "\n",
    "11. Tree-based : SelectFromModel\n",
    "\n",
    "12. Drawing a HeatMap of the correlation matrix (Tested)\n",
    "    \n",
    "    a. Pearson (Tested)\n",
    "    \n",
    "    b. Kendall (Tested)\n",
    "    \n",
    "    c. Spearman (Tested)\n",
    "    \n",
    "    \n",
    "13. Feature selection methods supported by scikit-learn http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Details:\n",
    "   \n",
    "\n",
    "## (7)\n",
    "\n",
    "### Relevant Files\n",
    "\n",
    "all_correlation_tests.ipynb\n",
    "\n",
    "100_correlation_tests.ipynb\n",
    "\n",
    "### Verdict:\n",
    "\n",
    "Correlation matrix was generated using Kendall, Spearman, and Pearson tests. Two of the assumptions behind calculating Pearson correlation scores is the fact that the distribution of both columns (that are being compared to calculate the score) must be normal and the \"noise\" (variance) between the independednt variables and the dependent variable is the same across all values of the independent variables. \n",
    "\n",
    "The first was tested by plotting a histogram of all the data points in a specific column. (Shown bellow) The data seems to be capped at the extremese (1 and 0) and the points in the middles looks relatively normally distributed with some skewness. I tried researching about if this violates pearson correlation assumptions but could not find any information on it\n",
    "\n",
    "![all histogram](../images/allhist.png)\n",
    "\n",
    "The second test was carried out by running a Levene Test that tests the null hypothesis that the input samples are from populations with equal variances (tests if the variance is uniformly equal). The lower the p value returned from the test, the weaker the possibility of the Null Hypothesis. The image bellow is a heatmap of the matrix of p values from a levene test between each column in the dataset: please note that the diagonal values are all filled with zeroes to place stronger emphasis on the levene tests between columns that are not equal to each other.\n",
    "\n",
    "![levene heatmap](../images/leveneheatmap.png)\n",
    "\n",
    "Majority of the data points are very close to 0, suggesting that this assumption behind the pearson correlation matrix fails.\n",
    "\n",
    "The Kendall and Spearman correlation tests do not have any assumptions, so there weren't any required tests\n",
    "\n",
    "## (12)\n",
    "\n",
    "### Relevant Files\n",
    "\n",
    "all_correlation_tests.ipynb\n",
    "\n",
    "100_correlation_tests.ipynb\n",
    "\n",
    "### Verdict:\n",
    "\n",
    "A heatmap of the correlation matrix was plotted to better visualize the relationship between the columns. The image bellow shoes the heatmap generated from pearson, kendall, and spearman correlation matrices, respectively:\n",
    "\n",
    "![pearson heatmap](../images/pearsoncorr.png)\n",
    "\n",
    "![kendall heatmap](../images/kendallcorr.png)\n",
    "\n",
    "![spearman heatmap](../images/spearmancorr.png)\n",
    "\n",
    "Looks like there the correlation value is high after every alternating 8 columns, regardless of which metric we used. (Although, the kendall correlation matrix heatmap seems to follow this pattern mroe closely). I tried grouping these columns together and decided to call it an \"octate group\" where each octate holds 15 columns. The following image dispalys a heatmap of the correlation matrix in each octate group:\n",
    "\n",
    "![octate heatmap](../images/corrmatrixbyoctate.png)\n",
    "\n",
    "\n",
    "Unlike the first huge matrix, there seems to be a lot more correlation going on when we group the columns into their respective octate group. Furthermore, we can take a look at the scatter plot displaying the relationship between the columns. The following image displays this exact relationship for the first octate group.\n",
    "\n",
    "![octate0 scatter](../images/octate0scatter.png)\n",
    "\n",
    "Unlike the following image (showing the sactter plot plotting the relationship between the first 10 columns), the image above has a lot more \"obvious\" relationships (some are non-linear), because we grouped the columns into octates.\n",
    "\n",
    "![first10cols scatter](../images/first10colcatter.png)\n",
    "\n",
    "Drawing out the heatmap of the correlation matrix suggests that we can go from 120 columns to 8 columns (which are storing the relationship between 15 columns in the relative octate group)\n",
    "\n",
    "\n",
    "## (8)\n",
    "\n",
    "### Relevant Files\n",
    "\n",
    "100_chisquared.ipynb\n",
    "\n",
    "### Verdict:\n",
    "\n",
    "30 minutes into working with chi squared tests, I realized that Chi square Test of independence can only be used to compare categorical variables\n",
    "\n",
    "**1 hour compiling summary**\n",
    "\n",
    "**Total time : 11.5 hours**\n",
    "\n",
    "1. (2 hrs) researching about feature selection\n",
    "2. (4 hours) playing around with top-100 csv file\n",
    "3. (.5 hours) researching about correlation metrics\n",
    "4. (1 hour) figuring out pearson correlation assumption tests\n",
    "5. (2.5 hours) carrying out all the tests done in top-100 csv file in the entire dataset\n",
    "6. (.5 hours) researching about chi2 feature selection\n",
    "7. (1 hour) compiling summary so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
