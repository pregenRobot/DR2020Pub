{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Researching about feature selection\n",
    "\n",
    "### Feature Selection vs Dimensionality Reduction\n",
    "\n",
    "Feature selection uses a subset of features from the orignal set in contrast to demnsionality reduction creating a new features based on the original set\n",
    "\n",
    "### Types of Feature Selection Methods\n",
    "\n",
    "#### Wrapper Feature Selection\n",
    "\"Create many models with different subsets of input features and select those featuers that result in the best performing model according to any performance metric\"\n",
    "\n",
    "#### Filter feature selection methods\n",
    "\"Use statistical techniques to evaluate the relationship between each input and target variable.\" Use these scores as the basis to choose (filter) those input variables that will be used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose statistical feature selection Method?\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "\n",
    "Use the flowchart in this page\n",
    "\n",
    "Our dataset (numerical input, categorical output) suggests that we try Kendall's and ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA -- correlation coefficient (linear)\n",
    "Kendall -- rank coefficient (nonliniear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with Wrapper Based Method\n",
    "\n",
    "Let's take a look at some of the possibly-helpful-for-our-current-task classifciation algorithms supported in scikit-learn that can be used for binary classification:\n",
    "\n",
    "Linear Models:\n",
    "1. Ridge Classification\n",
    "2. Lasso Classification\n",
    "3. Least Angle Regression\n",
    "4. Orthogonal Matching Pursuit\n",
    "3. Stochastic Gradient Descent Classifier\n",
    "4. Logistic Regression Classifier\n",
    "5. Support Vector Machine Classifiers\n",
    "6. Nearest Neighbors Classification (? -- will this work?) -- Checked does not work\n",
    "7. Gaussian Process Classification (never used it before) -- just checked, has no coef_ and thereby does not work\n",
    "10. Decision Trees (including random forrests and other ensemble methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: go through scikit learn's dimensionality reduction libraries (LDA, QDA, Shrinkage, and Estimation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: go through scikit learn's feature selection libraries \n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cosine.FATHER_FORENAME</th>\n",
       "      <th>Cosine.FATHER_SURNAME</th>\n",
       "      <th>Cosine.MOTHER_FORENAME</th>\n",
       "      <th>Cosine.MOTHER_MAIDEN_SURNAME</th>\n",
       "      <th>Cosine.PARENTS_PLACE_OF_MARRIAGE</th>\n",
       "      <th>Cosine.PARENTS_DAY_OF_MARRIAGE</th>\n",
       "      <th>Cosine.PARENTS_MONTH_OF_MARRIAGE</th>\n",
       "      <th>Cosine.PARENTS_YEAR_OF_MARRIAGE</th>\n",
       "      <th>Damerau-Levenshtein.FATHER_FORENAME</th>\n",
       "      <th>Damerau-Levenshtein.FATHER_SURNAME</th>\n",
       "      <th>...</th>\n",
       "      <th>Metaphone-Levenshtein.PARENTS_MONTH_OF_MARRIAGE</th>\n",
       "      <th>Metaphone-Levenshtein.PARENTS_YEAR_OF_MARRIAGE</th>\n",
       "      <th>NYSIIS-Levenshtein.FATHER_FORENAME</th>\n",
       "      <th>NYSIIS-Levenshtein.FATHER_SURNAME</th>\n",
       "      <th>NYSIIS-Levenshtein.MOTHER_FORENAME</th>\n",
       "      <th>NYSIIS-Levenshtein.MOTHER_MAIDEN_SURNAME</th>\n",
       "      <th>NYSIIS-Levenshtein.PARENTS_PLACE_OF_MARRIAGE</th>\n",
       "      <th>NYSIIS-Levenshtein.PARENTS_DAY_OF_MARRIAGE</th>\n",
       "      <th>NYSIIS-Levenshtein.PARENTS_MONTH_OF_MARRIAGE</th>\n",
       "      <th>NYSIIS-Levenshtein.PARENTS_YEAR_OF_MARRIAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.883</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cosine.FATHER_FORENAME  Cosine.FATHER_SURNAME  Cosine.MOTHER_FORENAME  \\\n",
       "0                   1.000                  1.000                   0.867   \n",
       "1                   0.000                  0.000                   0.408   \n",
       "2                   0.883                  1.000                   0.855   \n",
       "3                   0.000                  0.000                   0.000   \n",
       "4                   1.000                  0.707                   0.893   \n",
       "\n",
       "   Cosine.MOTHER_MAIDEN_SURNAME  Cosine.PARENTS_PLACE_OF_MARRIAGE  \\\n",
       "0                         1.000                               1.0   \n",
       "1                         0.195                               0.0   \n",
       "2                         1.000                               1.0   \n",
       "3                         0.195                               0.0   \n",
       "4                         0.939                               1.0   \n",
       "\n",
       "   Cosine.PARENTS_DAY_OF_MARRIAGE  Cosine.PARENTS_MONTH_OF_MARRIAGE  \\\n",
       "0                           1.000                             0.784   \n",
       "1                           0.000                             0.000   \n",
       "2                           0.784                             0.000   \n",
       "3                           0.000                             0.000   \n",
       "4                           1.000                             0.784   \n",
       "\n",
       "   Cosine.PARENTS_YEAR_OF_MARRIAGE  Damerau-Levenshtein.FATHER_FORENAME  \\\n",
       "0                            0.738                                0.800   \n",
       "1                            0.000                                0.000   \n",
       "2                            0.738                                0.833   \n",
       "3                            0.000                                0.000   \n",
       "4                            0.738                                0.800   \n",
       "\n",
       "   Damerau-Levenshtein.FATHER_SURNAME  ...  \\\n",
       "0                               0.875  ...   \n",
       "1                               0.000  ...   \n",
       "2                               0.875  ...   \n",
       "3                               0.000  ...   \n",
       "4                               0.800  ...   \n",
       "\n",
       "   Metaphone-Levenshtein.PARENTS_MONTH_OF_MARRIAGE  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   Metaphone-Levenshtein.PARENTS_YEAR_OF_MARRIAGE  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               0   \n",
       "4                                               0   \n",
       "\n",
       "   NYSIIS-Levenshtein.FATHER_FORENAME  NYSIIS-Levenshtein.FATHER_SURNAME  \\\n",
       "0                                 0.8                              0.857   \n",
       "1                                 0.0                              0.000   \n",
       "2                                 0.8                              0.800   \n",
       "3                                 0.0                              0.000   \n",
       "4                                 0.8                              0.800   \n",
       "\n",
       "   NYSIIS-Levenshtein.MOTHER_FORENAME  \\\n",
       "0                               0.833   \n",
       "1                               0.000   \n",
       "2                               0.833   \n",
       "3                               0.000   \n",
       "4                               0.833   \n",
       "\n",
       "   NYSIIS-Levenshtein.MOTHER_MAIDEN_SURNAME  \\\n",
       "0                                     0.857   \n",
       "1                                     0.000   \n",
       "2                                     0.857   \n",
       "3                                     0.000   \n",
       "4                                     0.857   \n",
       "\n",
       "   NYSIIS-Levenshtein.PARENTS_PLACE_OF_MARRIAGE  \\\n",
       "0                                         0.833   \n",
       "1                                         0.000   \n",
       "2                                         1.000   \n",
       "3                                         0.000   \n",
       "4                                         1.000   \n",
       "\n",
       "   NYSIIS-Levenshtein.PARENTS_DAY_OF_MARRIAGE  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "\n",
       "   NYSIIS-Levenshtein.PARENTS_MONTH_OF_MARRIAGE  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   NYSIIS-Levenshtein.PARENTS_YEAR_OF_MARRIAGE  \n",
       "0                                            0  \n",
       "1                                            0  \n",
       "2                                            0  \n",
       "3                                            0  \n",
       "4                                            0  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filepath = \"../data/UmeaSiblingBundlingMLDistances.csv\"\n",
    "\n",
    "all_data = pd.read_csv(data_filepath)\n",
    "\n",
    "data = all_data.iloc[:,0:-2]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier, Lasso, SGDClassifier, Lars, OrthogonalMatchingPursuit, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(data,all_data.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Classification\n",
    "NOTES:\n",
    "\n",
    "Adds a penalty to the size of coefficients in \"Ordinary Least Squares\" classification\n",
    "\n",
    "Usually a regression classifier but converts the binary targets to -1 and 1 and carries out a regression task for classifications\n",
    "\n",
    "##### Lasso Classification\n",
    "NOTES:\n",
    "\n",
    "Has a tendency to prefer solutions with fewer non-zero coefficients, effectively reducting the number of features upon which the given solution is dependent on. ( going to be useful for our feature selection task\n",
    "\n",
    "Might later use Elastic-Net (which uses both l1 and l2 regularization from Ridge and Lasso Classifcaiton, respectively)\n",
    "\n",
    "##### Least Angle Regression\n",
    "\n",
    "Regression algorithm for high dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani.\n",
    "\n",
    "\"At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.\"\n",
    "\n",
    "Looks quite promising for our task\n",
    "\n",
    "Combination of LassoLars also exists!\n",
    "\n",
    "##### Orthogonal Matching Pursuit\n",
    "\n",
    "\"Implements OMP algorithm for approximating the fit of a linear model with contraints imposed on the number of non-zero coefficinets\" (also called l0 norm apparently)\n",
    "\n",
    "Also is a \"forward feature selection method\" like Least Angle Regression (look into this a little bit more)\n",
    "\n",
    "\n",
    "Let's Test these out now\n",
    "\n",
    "##### SGD Classifer\n",
    "A standard popular classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgeclf = RidgeClassifier()\n",
    "ridgeclf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettopNfeatures(classifier,n):\n",
    "    topnindexes = []\n",
    "    if(classifier.coef_.shape == (1,120)):\n",
    "        coefficients = classifier.coef_[0].tolist()\n",
    "    else:\n",
    "        coefficients = classifier.coef_.tolist()\n",
    "    \n",
    "    for i in range(n):\n",
    "        index = np.argmax(np.abs(coefficients))\n",
    "        topnindexes.append(index)\n",
    "        coefficients = coefficients[:index]+coefficients[index+1:]\n",
    "\n",
    "    return topnindexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of top 12 features with the most extreme coefficients (more relevant). We can then use these columns for training the dataset again and checking out the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 22, 61, 0, 63, 70, 23, 38, 1, 13, 13, 68]\n"
     ]
    }
   ],
   "source": [
    "top12features = gettopNfeatures(ridgeclf,12)\n",
    "print(top12features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982243774890008\n",
      "0.9928696934153612\n"
     ]
    }
   ],
   "source": [
    "# ## Original score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ridgeclf = RidgeClassifier()\n",
    "ridgeclf.fit(X_train,y_train)\n",
    "y_pred = ridgeclf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# ## After carrying out feature selection\n",
    "\n",
    "ridgeclf_fs = RidgeClassifier()\n",
    "ridgeclf_fs.fit(X_train.iloc[:,top12features],y_train)\n",
    "y_pred_fs = ridgeclf_fs.predict(X_test.iloc[:,top12features])\n",
    "print(accuracy_score(y_test,y_pred_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is almost no difference between the 120 dimensional and 12 dimensional training data!\n",
    "Now let's repeat this process for the rest of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier\n",
      "No feature selection: 0.9982243774890008\n",
      "Important features:\n",
      "[7, 22, 61, 0, 63, 70, 23, 38, 1, 13, 13, 68]\n",
      "With feature selectrion: 0.9928696934153612\n",
      "==================\n",
      "Lasso\n",
      "No feature selection: -6.333769651423182e-06\n",
      "Important features:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "With feature selectrion: -6.333769651423182e-06\n",
      "==================\n",
      "SGDClassifier\n",
      "No feature selection: 0.9998470090264674\n",
      "Important features:\n",
      "[87, 82, 7, 22, 9, 31, 78, 28, 34, 13, 74, 15]\n",
      "With feature selectrion: 0.9999304586483942\n",
      "==================\n",
      "LogisticRegression\n",
      "No feature selection: 0.9999721834593577\n",
      "Important features:\n",
      "[87, 7, 22, 37, 14, 28, 42, 57, 80, 19, 49, 5]\n",
      "With feature selectrion: 0.9999675473692506\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "classifiers = [RidgeClassifier(),Lasso(),SGDClassifier(),LogisticRegression()]\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    classifier = clone(classifiers[i])\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    print(type(classifier).__name__)\n",
    "    print(\"No feature selection: \" + str(classifier.score(X_test,y_test)))\n",
    "    top12features = gettopNfeatures(classifier,12)\n",
    "    print(\"Important features:\")\n",
    "    print(top12features)\n",
    "    \n",
    "    classifier_fs = clone(classifiers[i])\n",
    "    classifier_fs.fit(X_train.iloc[:,top12features],y_train)\n",
    "    print(\"With feature selectrion: \" + str(classifier_fs.score(X_test.iloc[:,top12features],y_test)))\n",
    "    print(\"==================\")\n",
    "\n",
    "## Lars and OMP has the n_nonzero_coefs parameter which defines how many non zero coefficients we need. Therefore, we will carry out feature selection separately for these linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there aren't any single \"most informative\" feature. The LASSO classifier's result is also quit interesting. We need to dive deeper into why this is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=12,\n",
       "                          normalize=True, precompute='auto', tol=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larsclf = Lars(n_nonzero_coefs=12)\n",
    "ompclf = OrthogonalMatchingPursuit(n_nonzero_coefs=12)\n",
    "\n",
    "larsclf.fit(X_train,y_train)\n",
    "ompclf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lars score: 0.9712442122277875\n",
      "OMP score: 0.983855252911005\n"
     ]
    }
   ],
   "source": [
    "print(\"Lars score: \" + str(larsclf.score(X_test,y_test)))\n",
    "print(\"OMP score: \" + str(ompclf.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like LARS and OMP cannot compete with SGD and Ridge classifiers with feature selection.\n",
    "\n",
    "Let's carry out testing with non-linear-model classifiers as well\n",
    "\n",
    "NOTE: PERHAPS WE CAN CALCULATE AN INTERSECTION BETWEEN THE TOP12 most useful features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine classifiers\n",
    "NOTE:\n",
    "coef_ is only available in linear svm classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LinearSVC\n",
      "No feature selection: 0.9999860917296789\n",
      "Important features:\n",
      "[88, 56, 57, 85, 7, 85, 77, 22, 47, 14, 28, 35]\n",
      "[LibLinear]With feature selectrion: 0.9999675473692506\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svcclassifiers = [LinearSVC(verbose=True)]\n",
    "\n",
    "for i in range(len(svcclassifiers)):\n",
    "    classifier = clone(svcclassifiers[i])\n",
    "    \n",
    "    classifier.fit(X_train,y_train)\n",
    "    print(type(classifier).__name__)\n",
    "    print(\"No feature selection: \" + str(classifier.score(X_test,y_test)))\n",
    "    top12features = gettopNfeatures(classifier,12)\n",
    "    print(\"Important features:\")\n",
    "    print(top12features)\n",
    "    \n",
    "    classifier_fs = clone(svcclassifiers[i])\n",
    "    classifier_fs.fit(X_train.iloc[:,top12features],y_train)\n",
    "    print(\"With feature selectrion: \" + str(classifier_fs.score(X_test.iloc[:,top12features],y_test)))\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to test is with Decision Trees (and other tree/ensemble methods)\n",
    "\n",
    "**5 hours carrying out feature selection tests and research**\n",
    "\n",
    "**Total time : 17.5 hours**\n",
    "\n",
    "1. (2 hrs) researching about feature selection\n",
    "2. (4 hours) playing around with top-100 csv file\n",
    "3. (.5 hours) researching about correlation metrics\n",
    "4. (1 hour) figuring out pearson correlation assumption tests\n",
    "5. (2.5 hours) carrying out all the tests done in top-100 csv file in the entire dataset\n",
    "6. (.5 hours) researching about chi2 feature selection\n",
    "7. (1 hour) compiling summary so far\n",
    "8. (1 hour) visualizing statistics about each column\n",
    "9. 5 hours carrying out feature selection tests and research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Feature Selection Methods\n",
    "\n",
    "\n",
    "Two essentail types of feature selection\n",
    "\n",
    "Univariate --  treat each feature individually and independently of the feature space. This is how it functions in practice:\n",
    "It ranks features according to certain criteria.\n",
    "Then select the highest ranking features according to those criteria.\n",
    "One problem that can occur with univariate methods is they may select a redundant variable, as they donâ€™t take into consideration the relationship between features.\n",
    "\n",
    "Multivariate -- evaluate the entire feature space. They take into account features in relation to other ones in the dataset.\n",
    "These methods are able to handle duplicated, redundant, and correlated features.\n",
    "\n",
    "--------------Actual Methods------------------\n",
    "\n",
    "Basic Methods -- Remove columns that are either constant,quasi constant, or duplicates\n",
    "// Test what happens if we choose a single column with the largest variance from each octate\n",
    "\n",
    "Correlation -- use a single column from a range of columns that are correlated to an extent\n",
    "\n",
    "    Pearson Correlation\n",
    "    Kendall Correlation\n",
    "    Spearman Rank Coefficient\n",
    "    \n",
    "Statistical and Ranking Filter Methods -- tests that evaluate each feature individually. By shedding light on the target, they evaluate whether the variable is important in order to discriminate against the target.\n",
    "\n",
    "    Mutual Information -- measures the amount of information obtained about one variable through observing the other variable\n",
    "    Chi squared Test\n",
    "    ANOVA Univariate Test\n",
    "    \n",
    "Univariate ROC-AUC /RMSE -- Build a decision tree using a single variable and a target, rank features according to the ROC-AUC or RMSE, Select features with the highest ranking score\n",
    "\n",
    "(Can we use a decision tree to see what columns create the largest divisions high up in the tree?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far:\n",
    "\n",
    "Basic Method --We have already tried some of the basic methods and noticed that grouping columns by their respective octate group greatly reduces the column count while preseving the accuracy\n",
    "\n",
    "Different Correlation Methods -- We have tried using various correlation and rank coefficients to test which columns are correlated with what\n",
    "\n",
    "Statistical and Ranking Filter Methods -- isn't this essentialyl same as correlation methods?\n",
    "\n",
    "Univariate ROC-AUC / RMSE: Will work on this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ROC-AUC ? - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.\n",
    "\n",
    "\n",
    "Essentially -- this is just a fancy way of figruing out how many columns we need (although we might want to do this or something similar sometime later!!!)\n",
    "\n",
    "What's more interesting is the Decision Tree approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//What is upp with Lasso?\n",
    "\n",
    "//What is up with 7 that seems to appear a lot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
